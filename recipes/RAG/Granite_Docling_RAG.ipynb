{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U48hO1_V_JRG",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "# Building an AI-Powered Document Retrieval System with Docling and Granite\n",
        "\n",
        "*Using IBM Granite Models*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_eDALN1A9LF8"
      },
      "source": [
        "## Recipe Overview\n",
        "\n",
        "Welcome to this Granite recipe, in this recipe, you'll learn to harness the power of advanced tools to build AI-powered document retrieval systems. It will guide you through:\n",
        "\n",
        "- **Document Processing:** Learn to handle documents from various sources, parse and transform them into usable formats, and store them in vector databases using Docling.\n",
        "- **Retrieval-Augmented Generation (RAG):** Understand how to connect large language models (LLMs) like Granite with external knowledge bases to enhance query responses and generate valuable insights.\n",
        "- **LangChain for Workflow Integration:** Discover how to use LangChain to streamline and orchestrate document processing and retrieval workflows, enabling seamless interaction between different components of the system.\n",
        "\n",
        "This recipe leverages three cutting-edge technologies:\n",
        "\n",
        "1. **[Docling](https://docling-project.github.io/docling/):** An open-source toolkit for parsing and converting documents.\n",
        "2. **[Granite](https://www.ibm.com/granite/docs/models/granite/):** A state-of-the-art LLM available via an [API](https://www.ibm.com/topics/api) through Replicate, providing robust natural language capabilities.\n",
        "3. **[LangChain](https://github.com/langchain-ai/langchain):** A powerful framework for building applications powered by language models, designed to simplify complex workflows and integrate external tools seamlessly.\n",
        "\n",
        "By the end of this recipe, you will:\n",
        "- Gain proficiency in document processing and chunking.\n",
        "- Integrate vector databases to enhance retrieval capabilities.\n",
        "- Utilize RAG to perform efficient and accurate data retrieval for real-world applications.\n",
        "\n",
        "This recipe is designed for AI developers, researchers, and enthusiasts looking to enhance their knowledge of document management and advanced NLP techniques.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vooxv7ltEZBf"
      },
      "source": [
        "\n",
        "## Prerequisites\n",
        "\n",
        "- Familiarity with Python programming.\n",
        "- Basic understanding of large language models and natural language processing concepts.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PN2mK175_JRH",
        "jupyter": {
          "outputs_hidden": false
        },
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "## Step 1: Setting up the environment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4p_2cX1-_JRI",
        "jupyter": {
          "outputs_hidden": false
        },
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "Install dependencies."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "BfMWUUSs_JRI",
        "jupyter": {
          "outputs_hidden": false
        },
        "pycharm": {
          "name": "#%%\n"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c1f35348-c00e-4592-c482-ec4bdcbbda7f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "::group::Install Dependencies\n",
            "Collecting uv\n",
            "  Downloading uv-0.9.21-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
            "Downloading uv-0.9.21-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (22.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m22.2/22.2 MB\u001b[0m \u001b[31m89.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: uv\n",
            "Successfully installed uv-0.9.21\n",
            "\u001b[2mUsing Python 3.12.12 environment at: /usr\u001b[0m\n",
            "\u001b[2K\u001b[2mResolved \u001b[1m124 packages\u001b[0m \u001b[2min 5.98s\u001b[0m\u001b[0m\n",
            "\u001b[2K\u001b[2mPrepared \u001b[1m38 packages\u001b[0m \u001b[2min 3.05s\u001b[0m\u001b[0m\n",
            "\u001b[2mUninstalled \u001b[1m2 packages\u001b[0m \u001b[2min 8ms\u001b[0m\u001b[0m\n",
            "\u001b[2K\u001b[2mInstalled \u001b[1m38 packages\u001b[0m \u001b[2min 111ms\u001b[0m\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mcolorlog\u001b[0m\u001b[2m==6.10.1\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mdocling\u001b[0m\u001b[2m==2.66.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mdocling-core\u001b[0m\u001b[2m==2.57.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mdocling-ibm-models\u001b[0m\u001b[2m==3.10.3\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mdocling-parse\u001b[0m\u001b[2m==4.7.2\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mfaker\u001b[0m\u001b[2m==40.1.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mfiletype\u001b[0m\u001b[2m==1.2.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mibm-granite-community-utils\u001b[0m\u001b[2m==0.1.dev126 (from git+https://github.com/ibm-granite-community/utils@f602c014defb9560be93ab52ef9fa9dbbd4d3eae)\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mjsonlines\u001b[0m\u001b[2m==4.0.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mjsonref\u001b[0m\u001b[2m==1.1.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mlangchain-classic\u001b[0m\u001b[2m==1.0.1\u001b[0m\n",
            " \u001b[31m-\u001b[39m \u001b[1mlangchain-core\u001b[0m\u001b[2m==1.2.1\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mlangchain-core\u001b[0m\u001b[2m==1.2.6\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mlangchain-huggingface\u001b[0m\u001b[2m==1.2.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mlangchain-milvus\u001b[0m\u001b[2m==0.3.2\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mlangchain-replicate\u001b[0m\u001b[2m==0.1.dev26 (from git+https://github.com/ibm-granite-community/langchain-replicate.git@200c6f94a8c3bb59afc5dda0dfd88490cd5ba952)\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mlangchain-text-splitters\u001b[0m\u001b[2m==1.1.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mlatex2mathml\u001b[0m\u001b[2m==3.78.1\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mmarko\u001b[0m\u001b[2m==2.2.1\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mmilvus-lite\u001b[0m\u001b[2m==2.5.1\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mmpire\u001b[0m\u001b[2m==2.10.2\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mpolyfactory\u001b[0m\u001b[2m==3.2.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mpyclipper\u001b[0m\u001b[2m==1.4.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mpylatexenc\u001b[0m\u001b[2m==2.10\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mpymilvus\u001b[0m\u001b[2m==2.6.6\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mpypdfium2\u001b[0m\u001b[2m==4.30.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mpython-docx\u001b[0m\u001b[2m==1.2.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mpython-pptx\u001b[0m\u001b[2m==1.0.2\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mrapidocr\u001b[0m\u001b[2m==3.4.5\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mreplicate\u001b[0m\u001b[2m==1.0.7\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1msemchunk\u001b[0m\u001b[2m==2.2.2\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mtree-sitter\u001b[0m\u001b[2m==0.25.2\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mtree-sitter-c\u001b[0m\u001b[2m==0.24.1\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mtree-sitter-java\u001b[0m\u001b[2m==0.23.5\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mtree-sitter-javascript\u001b[0m\u001b[2m==0.25.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mtree-sitter-python\u001b[0m\u001b[2m==0.25.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mtree-sitter-typescript\u001b[0m\u001b[2m==0.23.2\u001b[0m\n",
            " \u001b[31m-\u001b[39m \u001b[1mtyper\u001b[0m\u001b[2m==0.20.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mtyper\u001b[0m\u001b[2m==0.19.2\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mxlsxwriter\u001b[0m\u001b[2m==3.2.9\u001b[0m\n",
            "::endgroup::\n"
          ]
        }
      ],
      "source": [
        "! echo \"::group::Install Dependencies\"\n",
        "%pip install uv\n",
        "! uv pip install git+https://github.com/ibm-granite-community/utils \\\n",
        "    transformers \\\n",
        "    langchain_classic \\\n",
        "    langchain_core \\\n",
        "    langchain_huggingface sentence_transformers \\\n",
        "    langchain_milvus 'pymilvus[milvus_lite]' \\\n",
        "    docling \\\n",
        "    'langchain_replicate @ git+https://github.com/ibm-granite-community/langchain-replicate.git'\n",
        "! echo \"::endgroup::\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4gu-Oeay_JRJ"
      },
      "source": [
        "## Step 2: Selecting System Components"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EDZd6WEf_JRJ"
      },
      "source": [
        "### Choose your Embeddings Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KFuZBhG-_JRJ"
      },
      "source": [
        "Specify the model to use for generating embedding vectors from text. Here we will be using one of the new [Granite Embeddings models](https://huggingface.co/collections/ibm-granite/granite-embedding-models-6750b30c802c1926a35550bb)\n",
        "\n",
        "To use a model from another provider, replace this code cell with one from [this Embeddings Model recipe](https://github.com/ibm-granite-community/granite-kitchen/blob/main/recipes/Components/Langchain_Embeddings_Models.ipynb)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "mvztNZly_JRJ"
      },
      "outputs": [],
      "source": [
        "from langchain_huggingface import HuggingFaceEmbeddings\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "embeddings_model_path = \"ibm-granite/granite-embedding-30m-english\"\n",
        "embeddings_model = HuggingFaceEmbeddings(\n",
        "    model_name=embeddings_model_path,\n",
        ")\n",
        "embeddings_tokenizer = AutoTokenizer.from_pretrained(embeddings_model_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ma8eWR10_JRJ"
      },
      "source": [
        "### Use the Granite model\n",
        "\n",
        "Select a Granite model from the [`ibm-granite`](https://replicate.com/ibm-granite) org on Replicate. Here we use the Replicate Langchain client to connect to the model.\n",
        "\n",
        "To get set up with Replicate, see [Getting Started with Replicate](https://github.com/ibm-granite-community/granite-kitchen/blob/main/recipes/Getting_Started/Getting_Started_with_Replicate.ipynb).\n",
        "\n",
        "To connect to a model on a provider other than Replicate, substitute this code cell with one from the [LLM component recipe](https://github.com/ibm-granite-community/granite-kitchen/blob/main/recipes/Components/Langchain_LLMs.ipynb)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "Ckyj7Zrh_JRK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "711d2fd8-e475-47c2-c9df-35be965233ca"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "REPLICATE_API_TOKEN loaded from Google Colab secret.\n"
          ]
        }
      ],
      "source": [
        "from langchain_replicate import ChatReplicate\n",
        "from ibm_granite_community.notebook_utils import get_env_var\n",
        "\n",
        "model_path = \"ibm-granite/granite-4.0-h-small\"\n",
        "model = ChatReplicate(\n",
        "    model=model_path,\n",
        "    replicate_api_token=get_env_var(\"REPLICATE_API_TOKEN\"),\n",
        "    model_kwargs={\n",
        "        \"max_tokens\": 1000, # Set the maximum number of tokens to generate as output.\n",
        "        \"min_tokens\": 100, # Set the minimum number of tokens to generate as output.\n",
        "    },\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "skQ8Xn0L_JRK"
      },
      "source": [
        "Now that we have the model downloaded, let's try asking it a question"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "-Sj3zZYE_JRK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c7109b67-b26c-4da5-8f03-419cf58efc38"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Both Bugatti and McLaren are renowned for their high-performance sports cars, but they cater to slightly different markets and have distinct characteristics.\n",
            "\n",
            "Bugatti is known for its ultra-luxury, high-performance vehicles. The most famous model, the Bugatti Veyron, was one of the fastest and most expensive cars in the world when it was introduced. The current model, the Bugatti Chiron, continues this tradition with a quad-turbocharged 8.0-liter W16 engine that produces 1500 horsepower. Bugatti cars are known for their top speed, luxury, and exclusivity.\n",
            "\n",
            "McLaren, on the other hand, is a British manufacturer known for its high-performance sports cars and involvement in Formula 1. McLaren cars are often lighter and more agile than their competitors, thanks to extensive use of carbon fiber in their construction. The McLaren 720S, for example, is a mid-engine sports car with a twin-turbocharged 4.0-liter V8 engine that produces 710 horsepower. McLaren cars are often praised for their handling, speed, and innovative technology.\n",
            "\n",
            "In conclusion, if you're looking for a car that offers extreme luxury and top speed, a Bugatti might be the right choice. If you prefer a car that offers high performance, agility, and innovative technology, a McLaren could be the better option. It ultimately depends on your personal preferences, budget, and what you value most in a car.\n"
          ]
        }
      ],
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "query = \"Buggati or Maclaren\"\n",
        "\n",
        "# Create a Granite prompt for question-answering\n",
        "prompt_template = ChatPromptTemplate.from_template(template=\"{input}\")\n",
        "\n",
        "chain = prompt_template | model\n",
        "\n",
        "output = chain.invoke({\"input\": query})\n",
        "\n",
        "print(output.text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C_2KpI91JhNH"
      },
      "source": [
        "Now, I know that UFC 310 happened in 2024, and this does not seem to be the right Pantoja. The model doesn't seem to know the answer but at least understands that this matchup did not occur. Let's see if it has some specific UFC rules info."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "_eR6qTQzMX_a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3715706c-b935-4f87-e177-d04afd05d9bf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "In the UFC, for non-championship fights, the weight allowance is typically 1 pound. This means that fighters are allowed to weigh up to 1 pound over the specified weight limit for their respective weight class on the official weigh-ins. However, if a fighter misses weight, they may be subject to penalties such as a percentage deduction from their fight purse going to their opponent, or in some cases, the fight may be cancelled. It's always best to check the specific rules and regulations for each event as they can sometimes vary.\n"
          ]
        }
      ],
      "source": [
        "query1 = \"How much weight allowance is allowed in non championship fights in the UFC?\"\n",
        "\n",
        "output = chain.invoke({\"input\": query1})\n",
        "\n",
        "print(output.text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LkmllCgNNlcF"
      },
      "source": [
        "Based on the official UFC rules, this is also incorrect. Let's try getting some documents that contains this information for the model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q2pGGBP7_JRJ"
      },
      "source": [
        "### Choose your Vector Database"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K4mMSwQx_JRJ"
      },
      "source": [
        "Specify the database to use for storing and retrieving embedding vectors.\n",
        "\n",
        "To connect to a vector database other than Milvus, replace this code cell with one from [this Vector Store recipe](https://github.com/ibm-granite-community/granite-kitchen/blob/main/recipes/Components/Langchain_Vector_Stores.ipynb)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MjaP-5Gp_JRJ"
      },
      "outputs": [],
      "source": [
        "from langchain_milvus import Milvus\n",
        "import tempfile\n",
        "\n",
        "db_file = tempfile.NamedTemporaryFile(prefix=\"milvus_\", suffix=\".db\", delete=False).name\n",
        "print(f\"The vector database will be saved to {db_file}\")\n",
        "\n",
        "vector_db = Milvus(\n",
        "    embedding_function=embeddings_model,\n",
        "    connection_args={\"uri\": db_file},\n",
        "    auto_id=True,\n",
        "    enable_dynamic_field=True,\n",
        "    index_params={\"index_type\": \"AUTOINDEX\"},\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nviHG3n7_JRK",
        "jupyter": {
          "outputs_hidden": false
        },
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "## Step 3: Building the Vector Database"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eZ7Guu7A_JRK"
      },
      "source": [
        "In this example, from a set of source documents, we use [Docling](https://docling-project.github.io/docling/) to convert the documents into text and then split the text into chunks, derive embedding vectors using the embedding model, and load it into the vector database. Creating this vector database will allow us to easily search across our documents, enabling us to use RAG."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HuB8kkzf_JRK"
      },
      "source": [
        "### Use Docling to download the documents, convert to text, and split into chunks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "se6So2yw_JRK"
      },
      "source": [
        "Here we have found a website that gives us information on UFC 310, as well as a PDF of the official UFC rules. Below, we will see that Docling can both convert and chunk the two documents."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YNGz_0gZ_JRK"
      },
      "outputs": [],
      "source": [
        "# Docling imports\n",
        "from docling.document_converter import DocumentConverter\n",
        "from docling_core.transforms.chunker.hybrid_chunker import HybridChunker\n",
        "from docling_core.types.doc.labels import DocItemLabel\n",
        "from langchain_core.documents import Document\n",
        "\n",
        "# Here are our documents, feel free to add more documents in formats that Docling supports\n",
        "sources = [\n",
        "    \"https://www.ufc.com/news/main-card-results-highlights-winner-interviews-ufc-310-pantoja-vs-asakura\",\n",
        "    \"https://media.ufc.tv/discover-ufc/Unified_Rules_MMA.pdf\",\n",
        "]\n",
        "\n",
        "converter = DocumentConverter()\n",
        "\n",
        "# Convert and chunk out documents\n",
        "doc_id = 0\n",
        "texts: list[Document] = [\n",
        "    Document(page_content=chunk.text, metadata={\"doc_id\": (doc_id:=doc_id+1), \"source\": source})\n",
        "    for source in sources\n",
        "    for chunk in HybridChunker(tokenizer=embeddings_tokenizer).chunk(converter.convert(source=source).document)\n",
        "    if any(filter(lambda c: c.label in [DocItemLabel.TEXT, DocItemLabel.PARAGRAPH], iter(chunk.meta.doc_items)))\n",
        "]\n",
        "\n",
        "print(f\"{len(texts)} document chunks created\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "htIYVVjHPKSX"
      },
      "outputs": [],
      "source": [
        "# Print all created documents\n",
        "for document in texts:\n",
        "    print(f\"Document ID: {document.metadata['doc_id']}\")\n",
        "    print(f\"Source: {document.metadata['source']}\")\n",
        "    print(f\"Content:\\n{document.page_content}\")\n",
        "    print(\"=\" * 80)  # Separator for clarity"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Bjz1IR3_JRK",
        "jupyter": {
          "outputs_hidden": false
        },
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "### Populate the vector database\n",
        "\n",
        "NOTE: Population of the vector database may take over a minute depending on your embedding model and service."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YSbVb6R4_JRK"
      },
      "outputs": [],
      "source": [
        "ids = vector_db.add_documents(texts)\n",
        "print(f\"{len(ids)} documents added to the vector database\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wq50gMAO_JRK"
      },
      "source": [
        "## Step 4: RAG with Granite"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uZOYJW0D_JRL"
      },
      "source": [
        "Now that we have succesfully converted our documents and vectorized them, we can set up out RAG pipeline."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UC_KSxFk_JRL"
      },
      "source": [
        "### Retrieve relevant chunks\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bf8T8eZk_JRL"
      },
      "source": [
        "Here we will test the as_retriever method to search through our newly created vector database for chunks that are relevant to our original query\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pMtTSHhQ_JRL"
      },
      "outputs": [],
      "source": [
        "retriever = vector_db.as_retriever()\n",
        "\n",
        "docs = retriever.invoke(query)\n",
        "print(docs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "viFYsnxTS2OC"
      },
      "source": [
        "Looks like it pulled some chunks that would have the information we are looking for. Let's go ahead and contruct our RAG pipeline."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CxVuFY_A_JRL"
      },
      "source": [
        "### Create the prompt for Granite"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cvoMcqtb_JRL"
      },
      "source": [
        "Next, we construct the prompt pipeline. This creates the prompt which holds the retrieved chunks from out previous search and feeds this to the model as context for answering our question."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PB-CPPTo_JRL"
      },
      "outputs": [],
      "source": [
        "from ibm_granite_community.langchain.chains.combine_documents import create_stuff_documents_chain\n",
        "from langchain_classic.chains.retrieval import create_retrieval_chain\n",
        "\n",
        "# Assemble the retrieval-augmented generation chain\n",
        "combine_docs_chain = create_stuff_documents_chain(\n",
        "    llm=model,\n",
        "    prompt=prompt_template,\n",
        ")\n",
        "rag_chain = create_retrieval_chain(\n",
        "    retriever=vector_db.as_retriever(),\n",
        "    combine_docs_chain=combine_docs_chain,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p_NU_Yhl_JRQ",
        "jupyter": {
          "outputs_hidden": false
        },
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "### Generate a retrieval-augmented response to a question"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qXQzDqAB_JRQ",
        "jupyter": {
          "outputs_hidden": false
        },
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "The pipeline uses the query to locate documents from the vector database and use them as context for the query."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hdo9PXsU_JRQ",
        "jupyter": {
          "outputs_hidden": false
        },
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "output = rag_chain.invoke({\"input\": query})\n",
        "\n",
        "print(output['answer'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KeErN8ZVPgf6"
      },
      "source": [
        "Awesome! It looks like the model figured out our first question. Let's see if it figure out the rule we were looking for."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F5u06aEjThd3"
      },
      "outputs": [],
      "source": [
        "output = rag_chain.invoke({\"input\": query1})\n",
        "\n",
        "print(output['answer'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c_uyUdCu_JRQ"
      },
      "source": [
        "Awesome! We can now see that we have created a pipeline that can successfully leverage knowledge from multiple document types for generation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GTaX1-SPT6rU"
      },
      "source": [
        "## Next Steps\n",
        "\n",
        "- Explore advanced RAG workflows for other industries\n",
        "- Experiment with other document types and larger datasets.\n",
        "- Optimize prompt engineering for better Granite responses.\n",
        "\n",
        "Thank you for using this recipe!"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}